{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "5mNJ45hG9AxS",
        "outputId": "856b15b7-7141-4aa7-95eb-72a00fcf0de9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (ipython-input-13-982031627.py, line 11)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-13-982031627.py\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    df = pd.read_csv(\"C:\\Users\\User\\Desktop\\cloud_services_reviews.csv\")\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load sample data\n",
        "# Assuming data has columns: 'service_name', 'review', 'cost', 'latency', 'availability', 'scalability'\n",
        "df = pd.read_csv(\"cloud_services_reviews.csv\")\n",
        "\n",
        "# Initialize RoBERTa tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "# Function to extract embeddings\n",
        "def get_roberta_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=64)\n",
        "    with torch.no_grad():\n",
        "        outputs = roberta_model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # [CLS] token\n",
        "\n",
        "# Generate embeddings for reviews\n",
        "print(\"Extracting RoBERTa embeddings...\")\n",
        "df['embedding'] = df['review'].apply(get_roberta_embedding)\n",
        "\n",
        "# Normalize numerical criteria\n",
        "criteria_cols = ['cost', 'latency', 'availability', 'scalability']\n",
        "scaler = MinMaxScaler()\n",
        "df[criteria_cols] = scaler.fit_transform(df[criteria_cols])\n",
        "\n",
        "# Prepare features and labels for DL model\n",
        "X_text = np.stack(df['embedding'].values)\n",
        "X_numeric = df[criteria_cols].values\n",
        "X_combined = np.hstack([X_text, X_numeric])\n",
        "y = df[['availability', 'scalability']]  # Target could be composite or multi-output\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y.values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define simple Deep Neural Network\n",
        "class CloudRankNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(CloudRankNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu1(self.fc1(x))\n",
        "        out = self.relu2(self.fc2(out))\n",
        "        return self.fc3(out)\n",
        "\n",
        "# Model training\n",
        "model = CloudRankNN(input_size=X_combined.shape[1], output_size=y.shape[1])\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Convert data to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "# Train loop\n",
        "print(\"Training deep learning model...\")\n",
        "for epoch in range(30):\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"Epoch [{epoch}/30], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Get predictions for MCDA\n",
        "X_all_tensor = torch.tensor(X_combined, dtype=torch.float32)\n",
        "predicted_scores = model(X_all_tensor).detach().numpy()\n",
        "\n",
        "# Add prediction as new criteria for MCDA\n",
        "df['predicted_score'] = predicted_scores.mean(axis=1)\n",
        "\n",
        "# ------- MCDA using TOPSIS ----------\n",
        "def topsis(matrix, weights, impacts):\n",
        "    norm_matrix = matrix / np.sqrt((matrix ** 2).sum(axis=0))\n",
        "    weighted_matrix = norm_matrix * weights\n",
        "\n",
        "    ideal_best = np.max(weighted_matrix, axis=0) if impacts == '+' else np.min(weighted_matrix, axis=0)\n",
        "    ideal_worst = np.min(weighted_matrix, axis=0) if impacts == '+' else np.max(weighted_matrix, axis=0)\n",
        "\n",
        "    dist_best = np.sqrt(((weighted_matrix - ideal_best) ** 2).sum(axis=1))\n",
        "    dist_worst = np.sqrt(((weighted_matrix - ideal_worst) ** 2).sum(axis=1))\n",
        "\n",
        "    score = dist_worst / (dist_best + dist_worst)\n",
        "    return score\n",
        "\n",
        "# Apply TOPSIS with weights and impact directions\n",
        "criteria_matrix = df[['cost', 'latency', 'availability', 'scalability', 'predicted_score']].values\n",
        "weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Equal weights (customizable)\n",
        "impacts = np.array(['-', '-', '+', '+', '+'])  # cost and latency are negatives\n",
        "\n",
        "# Convert impact to numerical mask\n",
        "impacts_mask = np.array([1 if i == '+' else -1 for i in impacts])\n",
        "topsis_scores = topsis(criteria_matrix, weights, impacts_mask)\n",
        "\n",
        "# Add ranking to dataframe\n",
        "df['rank_score'] = topsis_scores\n",
        "df = df.sort_values(by='rank_score', ascending=False)\n",
        "\n",
        "# Final output\n",
        "print(\"\\nTop Cloud Services based on Hybrid RoBERTa + MCDA Ranking:\")\n",
        "print(df[['service_name', 'rank_score']].head(10))\n"
      ]
    }
  ]
}